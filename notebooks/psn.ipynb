{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import anndata as ad\n",
    "import umap\n",
    "import scipy as sip\n",
    "from scipy import sparse\n",
    "from sklearn import preprocessing\n",
    "import scanpy as scp\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import seaborn as sb\n",
    "import snf\n",
    "import sklearn as skl\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import adjusted_mutual_info_score as ami\n",
    "from snf import compute, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import math\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = os.path.abspath(os.path.join(os.getcwd(),\"../data\"))\n",
    "\n",
    "input = os.path.join(data, \"input\")\n",
    "output = os.path.join(data, \"output\")\n",
    "\n",
    "complementary = os.path.join(input, \"complementary\")\n",
    "\n",
    "shared = os.path.join(output, \"shared_info_74\")\n",
    "\n",
    "\n",
    "level1 = os.path.join(output, \"level1\")\n",
    "level2 = os.path.join(output, \"level2\")\n",
    "level3 = os.path.join(output, \"level3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(level1, \"graphs_74.pickle\"), \"rb\") as f:\n",
    "    level1_graphs = pickle.load(f) \n",
    "with open(os.path.join(level2, \"graphs_74.pickle\"), \"rb\") as f:\n",
    "    level2_graphs = pickle.load(f) \n",
    "with open(os.path.join(level3, \"graphs_74.pickle\"), \"rb\") as f:\n",
    "    level3_graphs = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose between [\"feature_matrices_standard\", \"feature_matrices\", \"feature_matrices_quant_N\", \"feature_matrices_quant_U\", \"feature_matrices_minmax\", \"feature_matrices_maxabs\"]. \n",
    "with open(os.path.join(level1, \"feature_matrices_standard.pickle\"), \"rb\") as f:\n",
    "    level1_fm = pickle.load(f) \n",
    "with open(os.path.join(level2, \"feature_matrices_standard.pickle\"), \"rb\") as f:\n",
    "    level2_fm = pickle.load(f) \n",
    "with open(os.path.join(level3, \"feature_matrices_standard.pickle\"), \"rb\") as f:\n",
    "    level3_fm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = \"standard\" #Choose between [\"standard\", \"wot\", \"quant_N\", \"quant_U\", \"minmax\", \"maxabs\"]. It should match the feature_matrix suffix above e.g. feature_matrices_standard goes with tr = standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(shared, \"patients_74.pickle\"), \"rb\") as f:\n",
    "    patients_74 = pickle.load(f) \n",
    "\n",
    "with open(os.path.join(complementary, \"module_name_to_id.pickle\"), \"rb\") as f:\n",
    "    module_name_to_id = pickle.load(f) \n",
    "with open(os.path.join(complementary, \"id_to_module_name.pickle\"), \"rb\") as f:\n",
    "    id_to_module_name = pickle.load(f) \n",
    "\n",
    "with open(os.path.join(shared, \"id_to_modality.pickle\"), \"rb\") as f:\n",
    "    id_to_modality = pickle.load(f) \n",
    "with open(os.path.join(shared, \"modality_to_id.pickle\"), \"rb\") as f:\n",
    "    modality_to_id = pickle.load(f) \n",
    "\n",
    "with open(os.path.join(shared, \"id_to_name.pickle\"), \"rb\") as f:\n",
    "    id_to_name = pickle.load(f) \n",
    "with open(os.path.join(shared, \"name_to_id.pickle\"), \"rb\") as f:\n",
    "    name_to_id = pickle.load(f) \n",
    "\n",
    "with open(os.path.join(level1, \"noise_graphs_74.pickle\"), \"rb\") as f:\n",
    "    noise_74 = pickle.load(f) \n",
    "\n",
    "with open(os.path.join(shared, \"noise_name_to_id.pickle\"), \"rb\") as f:\n",
    "    noise_name_to_id = pickle.load(f) \n",
    "with open(os.path.join(shared, \"noise_id_to_name.pickle\"), \"rb\") as f:\n",
    "    noise_id_to_name = pickle.load(f) \n",
    "\n",
    "with open(os.path.join(shared, \"ground_truth_d.pickle\"), \"rb\") as f:\n",
    "    gt_d = pickle.load(f) \n",
    "with open(os.path.join(shared, \"ground_truth_dg.pickle\"), \"rb\") as f:\n",
    "    gt_dg = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = list(level1_graphs[\"G0\"].obs.Disease.unique())\n",
    "disease_groups = list(level1_graphs[\"G0\"].obs.DiseaseGroup.unique())\n",
    "\n",
    "with open(os.path.join(shared, 'diseases.pickle'), 'wb') as f:\n",
    "        pickle.dump(diseases, f)\n",
    "\n",
    "with open(os.path.join(shared, 'disease_groups.pickle'), 'wb') as f:\n",
    "        pickle.dump(disease_groups, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every modality, extract the set of graphs belonging to it\n",
    "citeRNA_g_with_74 = {}\n",
    "bulkRNA_g_with_74 = {}\n",
    "luminex_g_with_74 = {}\n",
    "cytof_g_with_74 = {}\n",
    "adt_g_with_74 = {}\n",
    "facs_g_with_74 = {}\n",
    "for id, g in level1_graphs.items():\n",
    "    modality = g.uns[\"modality\"]\n",
    "    if modality == \"citeRNA\":\n",
    "        citeRNA_g_with_74[id] = g\n",
    "    elif modality == \"bulkRNA\":\n",
    "        bulkRNA_g_with_74[id] = g\n",
    "    elif modality == \"cytof\":\n",
    "        cytof_g_with_74[id] = g\n",
    "    elif modality == \"luminex\":\n",
    "        luminex_g_with_74[id] = g\n",
    "    elif modality == \"facs\":\n",
    "        facs_g_with_74[id] = g\n",
    "    elif modality == \"adt\":\n",
    "        adt_g_with_74[id] = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_patients = [\n",
    "\"S00027-Ja003\",\n",
    "\"S00033-Ja003\",\n",
    "\"S00033-Ja005\",\n",
    "\"S00068-Ja005\",\n",
    "\"S00095-Ja005\",\n",
    "\"S00065-Ja003\",\n",
    "\"S00005-Ja005\",\n",
    "\"S00040-Ja005\",\n",
    "\"S00119-Ja003\",\n",
    "\"S00020-Ja003\"]\n",
    "interesting_patients = set([p.split(\"-\")[0] for p in interesting_patients]) # Extract ids, remove duplicates\n",
    "interesting_patients = list(set(patients_74).intersection(interesting_patients))\n",
    "\n",
    "\n",
    "healthy_patients = [p for p in list(level1_graphs[\"G0\"].obs_names) if level1_graphs[\"G0\"][p].obs.Disease[0] == \"HV\"]\n",
    "\n",
    "\n",
    "palette = ig.RainbowPalette(n=7)\n",
    "disease_color_map = {diseases[i]:palette[i] for i in range(6)}\n",
    "palette = ig.RainbowPalette(n=3)\n",
    "disease_group_color_map = {disease_groups[i]:palette[i] for i in range(3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Panelize-Reward-score checks the alignment of edge weights with the random-walker objective. In an ideal case, a random-walker transitions whithin the same disease group. Hence, the edge weights between patients with similar diseases are rewarded with +1. Weights between different disease are panelized with -1. Weights between healthy and patient are panelized with -2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "panelize_reward_d = []\n",
    "panelize_reward_dg = []\n",
    "\n",
    "for i in patients_74:\n",
    "    panelize_reward_d_ij = []\n",
    "    panelize_reward_dg_ij = []\n",
    "\n",
    "    d1 = level1_graphs[\"G0\"][i].obs.Disease[0]\n",
    "    dg1 = level1_graphs[\"G0\"][i].obs.DiseaseGroup[0]\n",
    "\n",
    "    for j in patients_74:\n",
    "        d2 = level1_graphs[\"G0\"][j].obs.Disease[0]\n",
    "        dg2 = level1_graphs[\"G0\"][j].obs.DiseaseGroup[0]\n",
    "\n",
    "        if d1 == d2:\n",
    "            panelize_reward_d_ij.append(1)\n",
    "        else:\n",
    "            panelize_reward_d_ij.append(-1)\n",
    "        \"\"\"elif dg1 == dg2:\n",
    "            p_d_ij.append(0.5)\n",
    "        elif d1 == \"HV\" or d2 == \"HV\":\n",
    "            p_d_ij.append(-1)\n",
    "        else:\n",
    "            p_d_ij.append(-0.5)\"\"\"\n",
    "\n",
    "        if dg1 == dg2:\n",
    "            panelize_reward_dg_ij.append(1)\n",
    "        elif dg1 == \"Healthy\" or dg2 == \"Healthy\":\n",
    "            panelize_reward_dg_ij.append(-2)\n",
    "        else:\n",
    "            panelize_reward_dg_ij.append(-1)\n",
    "\n",
    "    panelize_reward_d.append(panelize_reward_d_ij)\n",
    "    panelize_reward_dg.append(panelize_reward_dg_ij)\n",
    "\n",
    "\n",
    "for i in range(len(panelize_reward_dg)):\n",
    "    panelize_reward_d[i][i] = 0\n",
    "    panelize_reward_dg[i][i] = 0\n",
    "\n",
    "panelize_reward_d = np.array(panelize_reward_d, dtype=\"float64\")\n",
    "panelize_reward_dg = np.array(panelize_reward_dg, dtype=\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [1. 0. 1. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "optimal_graph = []\n",
    "for i, dg1 in enumerate(list(level1_graphs[\"G0\"].obs.DiseaseGroup)):\n",
    "    neighbors_of_i = []\n",
    "    for j, dg2 in enumerate(list(level1_graphs[\"G0\"].obs.DiseaseGroup)):\n",
    "        if i == j or dg1 != dg2:\n",
    "            neighbors_of_i.append(0)\n",
    "        else:\n",
    "            neighbors_of_i.append(1)\n",
    "    optimal_graph.append(neighbors_of_i)\n",
    "    \n",
    "optimal_graph = np.array(optimal_graph, dtype = \"float64\")\n",
    "print(optimal_graph)\n",
    "\n",
    "optimal_rw_score = 74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_frequency_weights = {\"citeRNA\": 1/209, \"bulkRNA\": 1/12, \"adt\": 1/11, \"facs\": 1, \"luminex\": 1, \"cytof\": 1/12 }\n",
    "modality_frequency_weights  = {id: modality_frequency_weights[g.uns[\"modality\"]] * 1/ 6 for id, g in level1_graphs.items()}\n",
    "weighted_avg_weights = {id: 1/len(level1_graphs)  for id, g in level1_graphs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transition_matrix(fused_network):\n",
    "    tm = []\n",
    "    for i, row in enumerate(fused_network):\n",
    "        tm.append(row/ sum(row))\n",
    "    return tm\n",
    "\n",
    "def random_walker_objective(transition_matrix):\n",
    "    d_score = (transition_matrix * panelize_reward_d).sum()\n",
    "    dg_score = (transition_matrix * panelize_reward_dg).sum()\n",
    "    #score = 0.5 * d_score + 0.5 * dg_score\n",
    "    score =  dg_score\n",
    "    return d_score, dg_score, score\n",
    "\n",
    "def adjust_weights(psns, scale_weights = modality_frequency_weights):\n",
    "    adjusted_weights = {}\n",
    "    for id, psn in psns.items():\n",
    "        adjusted_weights[id] = psn * modality_frequency_weights[id]\n",
    "    return adjusted_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(g, k, metric): # g is an AnnData Object\n",
    "    new_g = g.copy()\n",
    "    if not tr == \"wot\":\n",
    "        new_g.X = new_g.layers[tr]\n",
    "    scp.pp.neighbors(new_g, n_neighbors = k, use_rep='X', metric =  metric) \n",
    "    return np.array(sparse.csr_matrix.todense(new_g.obsp[\"connectivities\"]), dtype = \"float64\") # Returns a matrix\n",
    "\n",
    "def aff(g, metric, k, mu ): # g is a list of matrices \n",
    "    return compute.make_affinity(g, metric=metric, K=k, mu=mu, normalize = False)  # Returns a list of matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing PSNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setup_knn_pr = {}\\nsetup_knn_s = {}\\nsetup_knn_db = {}\\n\\nks = [4,5,6,7,8]\\nmetrics = [\"cosine\", \"euclidean\", \"sqeuclidean\", \"correlation\", \"seuclidean\", \"minkowski\", \"chebyshev\", \"canberra\"]\\ncombinations = set(itertools.product(ks, metrics))\\n\\nfor i, combi in enumerate(combinations):\\n    print(str(i), \"/\", str(len(combinations)))\\n    try:\\n        k = combi[0]\\n        metric = combi[1]\\n        graphs = {}\\n        fused = None\\n        for id, g in level1_graphs.items():\\n            weight = modality_frequency_weights[id]\\n            graphs[id] = knn(g, k, metric)\\n            \\n            if fused is None:\\n                fused = weight * graphs[id]\\n            else:\\n                fused = weight * graphs[id] + fused\\n            \\n       \\n        transition = make_transition_matrix(fused)\\n        pr_score = random_walker_objective(transition)[2]\\n\\n        ad_obj = ad.AnnData(fused)\\n        ad_obj.obsp[\\'connectivities\\'] = fused\\n        scp.tl.leiden(ad_obj, resolution=1, key_added=\\'leiden\\', adjacency=fused, directed=False, use_weights=True)\\n        l = list(ad_obj.obs.leiden)\\n\\n        s_score = skl.metrics.silhouette_score(fused, l)\\n        db_score = skl.metrics.davies_bouldin_score(fused, l)\\n   \\n    \\n        print(\"success\")\\n        setup_knn_pr[pr_score] = {\"k\":k, \"metric\": metric}\\n        setup_knn_s[s_score] = {\"k\":k, \"metric\": metric}\\n        setup_knn_db[db_score] = {\"k\":k, \"metric\": metric}\\n\\n    except np.linalg.LinAlgError:\\n        print(i, \"fail\")\\n        continue\\n    except BaseException:\\n        print(i, \"fail\")\\n        continue'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment if you want to optimize the hyperparameters of WA\n",
    "\n",
    "\"\"\"setup_knn_pr = {}\n",
    "setup_knn_s = {}\n",
    "setup_knn_db = {}\n",
    "\n",
    "ks = [4,5,6,7,8]\n",
    "metrics = [\"cosine\", \"euclidean\", \"sqeuclidean\", \"correlation\", \"seuclidean\", \"minkowski\", \"chebyshev\", \"canberra\"]\n",
    "combinations = set(itertools.product(ks, metrics))\n",
    "\n",
    "for i, combi in enumerate(combinations):\n",
    "    print(str(i), \"/\", str(len(combinations)))\n",
    "    try:\n",
    "        k = combi[0]\n",
    "        metric = combi[1]\n",
    "        graphs = {}\n",
    "        fused = None\n",
    "        for id, g in level1_graphs.items():\n",
    "            weight = modality_frequency_weights[id]\n",
    "            graphs[id] = knn(g, k, metric)\n",
    "            \n",
    "            if fused is None:\n",
    "                fused = weight * graphs[id]\n",
    "            else:\n",
    "                fused = weight * graphs[id] + fused\n",
    "            \n",
    "       \n",
    "        transition = make_transition_matrix(fused)\n",
    "        pr_score = random_walker_objective(transition)[2]\n",
    "\n",
    "        ad_obj = ad.AnnData(fused)\n",
    "        ad_obj.obsp['connectivities'] = fused\n",
    "        scp.tl.leiden(ad_obj, resolution=1, key_added='leiden', adjacency=fused, directed=False, use_weights=True)\n",
    "        l = list(ad_obj.obs.leiden)\n",
    "\n",
    "        s_score = skl.metrics.silhouette_score(fused, l)\n",
    "        db_score = skl.metrics.davies_bouldin_score(fused, l)\n",
    "   \n",
    "    \n",
    "        print(\"success\")\n",
    "        setup_knn_pr[pr_score] = {\"k\":k, \"metric\": metric}\n",
    "        setup_knn_s[s_score] = {\"k\":k, \"metric\": metric}\n",
    "        setup_knn_db[db_score] = {\"k\":k, \"metric\": metric}\n",
    "\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(i, \"fail\")\n",
    "        continue\n",
    "    except BaseException:\n",
    "        print(i, \"fail\")\n",
    "        continue\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setup_knn_pr = {k: v for k, v in sorted(setup_knn_pr.items(), key=lambda item: item[0])} # sort according to scores\\nbest_knn_pr = setup_knn_pr[list(setup_knn_pr.keys())[-1]]\\nsetup_knn_s = {k: v for k, v in sorted(setup_knn_s.items(), key=lambda item: item[0])} \\nbest_knn_s = setup_knn_s[list(setup_knn_s.keys())[-1]]\\nsetup_knn_db = {k: v for k, v in sorted(setup_knn_db.items(), key=lambda item: item[0])} \\nbest_knn_db = setup_knn_db[list(setup_knn_db.keys())[0]]\\n\\nprint(best_knn_pr, best_knn_s, best_knn_db)'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment if you want to optimize the hyperparameters of WA\n",
    "\n",
    "\"\"\"setup_knn_pr = {k: v for k, v in sorted(setup_knn_pr.items(), key=lambda item: item[0])} # sort according to scores\n",
    "best_knn_pr = setup_knn_pr[list(setup_knn_pr.keys())[-1]]\n",
    "setup_knn_s = {k: v for k, v in sorted(setup_knn_s.items(), key=lambda item: item[0])} \n",
    "best_knn_s = setup_knn_s[list(setup_knn_s.keys())[-1]]\n",
    "setup_knn_db = {k: v for k, v in sorted(setup_knn_db.items(), key=lambda item: item[0])} \n",
    "best_knn_db = setup_knn_db[list(setup_knn_db.keys())[0]]\n",
    "\n",
    "print(best_knn_pr, best_knn_s, best_knn_db)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_knn = {'k': 7, 'metric': 'correlation'} # Comment out if you are using optimal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setup_kernel_pr = {}\\nsetup_kernel_s = {}\\nsetup_kernel_db = {}\\n\\nks = [4,5,6,7,8]\\nmetrics = [\"cosine\", \"euclidean\", \"sqeuclidean\", \"correlation\", \"seuclidean\", \"minkowski\", \"mahalanobis\", \"chebyshev\", \"canberra\"]\\nmus = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\\n\\ncombinations = set(itertools.product(ks, metrics, mus))\\n\\nfor i, combi in enumerate(combinations):\\n    print(str(i), \"/\", str(len(combinations)))\\n    try:\\n        k = combi[0]\\n        metric = combi[1]\\n        mu = combi[2] \\n        \\n        graphs = aff(list(level1_fm.values()), metric, k, mu)\\n        fused = compute.snf(graphs, K=k, t = 40)\\n\\n        transition = make_transition_matrix(fused)\\n        score_pr = random_walker_objective(transition)[2]\\n\\n        \\n        first, second = compute.get_n_clusters(fused)\\n        l = cluster.spectral_clustering(fused, n_clusters=first, n_init = 40)\\n\\n        score_s = skl.metrics.silhouette_score(fused, l)\\n        score_db = skl.metrics.davies_bouldin_score(fused, l)\\n\\n        print(\"success\")\\n        setup_kernel_pr[score_pr] = {\"k\":k, \"metric\": metric, \"mu\": mu}\\n        setup_kernel_s[score_s] = {\"k\":k, \"metric\": metric, \"mu\": mu}\\n        setup_kernel_db[score_db] = {\"k\":k, \"metric\": metric, \"mu\": mu}\\n        \\n    except ValueError:\\n        print(i, \"fail\")\\n        continue'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment if you want to optiimize the hyperparameters of SNF\n",
    "\n",
    "\"\"\"setup_kernel_pr = {}\n",
    "setup_kernel_s = {}\n",
    "setup_kernel_db = {}\n",
    "\n",
    "ks = [4,5,6,7,8]\n",
    "metrics = [\"cosine\", \"euclidean\", \"sqeuclidean\", \"correlation\", \"seuclidean\", \"minkowski\", \"mahalanobis\", \"chebyshev\", \"canberra\"]\n",
    "mus = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "combinations = set(itertools.product(ks, metrics, mus))\n",
    "\n",
    "for i, combi in enumerate(combinations):\n",
    "    print(str(i), \"/\", str(len(combinations)))\n",
    "    try:\n",
    "        k = combi[0]\n",
    "        metric = combi[1]\n",
    "        mu = combi[2] \n",
    "        \n",
    "        graphs = aff(list(level1_fm.values()), metric, k, mu)\n",
    "        fused = compute.snf(graphs, K=k, t = 40)\n",
    "\n",
    "        transition = make_transition_matrix(fused)\n",
    "        score_pr = random_walker_objective(transition)[2]\n",
    "\n",
    "        \n",
    "        first, second = compute.get_n_clusters(fused)\n",
    "        l = cluster.spectral_clustering(fused, n_clusters=first, n_init = 40)\n",
    "\n",
    "        score_s = skl.metrics.silhouette_score(fused, l)\n",
    "        score_db = skl.metrics.davies_bouldin_score(fused, l)\n",
    "\n",
    "        print(\"success\")\n",
    "        setup_kernel_pr[score_pr] = {\"k\":k, \"metric\": metric, \"mu\": mu}\n",
    "        setup_kernel_s[score_s] = {\"k\":k, \"metric\": metric, \"mu\": mu}\n",
    "        setup_kernel_db[score_db] = {\"k\":k, \"metric\": metric, \"mu\": mu}\n",
    "        \n",
    "    except ValueError:\n",
    "        print(i, \"fail\")\n",
    "        continue\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setup_kernel_pr = {k: v for k, v in sorted(setup_kernel_pr.items(), key=lambda item: item[0])} # sort according to scores\\nbest_kernel_pr = setup_kernel_pr[list(setup_kernel_pr.keys())[-1]]\\nsetup_kernel_s = {k: v for k, v in sorted(setup_kernel_s.items(), key=lambda item: item[0])} \\nbest_kernel_s = setup_kernel_s[list(setup_kernel_s.keys())[-1]]\\nsetup_kernel_db = {k: v for k, v in sorted(setup_kernel_db.items(), key=lambda item: item[0])} \\nbest_kernel_db = setup_kernel_db[list(setup_kernel_db.keys())[0]]\\nprint(best_kernel_pr,best_kernel_s, best_kernel_db )'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment if you want to optiimize the hyperparameters of SNF\n",
    "\n",
    "\"\"\"setup_kernel_pr = {k: v for k, v in sorted(setup_kernel_pr.items(), key=lambda item: item[0])} # sort according to scores\n",
    "best_kernel_pr = setup_kernel_pr[list(setup_kernel_pr.keys())[-1]]\n",
    "setup_kernel_s = {k: v for k, v in sorted(setup_kernel_s.items(), key=lambda item: item[0])} \n",
    "best_kernel_s = setup_kernel_s[list(setup_kernel_s.keys())[-1]]\n",
    "setup_kernel_db = {k: v for k, v in sorted(setup_kernel_db.items(), key=lambda item: item[0])} \n",
    "best_kernel_db = setup_kernel_db[list(setup_kernel_db.keys())[0]]\n",
    "print(best_kernel_pr,best_kernel_s, best_kernel_db )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_kernel = {'k': 7, 'metric': 'sqeuclidean', 'mu': 0.3} # comment out if you are using optimal values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given Level 1 pseudobulks, generate noise and non-noise knn-based PSNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shakiba/Desktop/thesis.tmp/code/aggregated_patient_info/aggregated_patient_info/lib/python3.11/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/Users/shakiba/Desktop/thesis.tmp/code/aggregated_patient_info/aggregated_patient_info/lib/python3.11/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "level1_knn_psns = {}\n",
    "for id, g in level1_graphs.items():\n",
    "    graph = knn(g, best_knn[\"k\"], best_knn[\"metric\"])\n",
    "    level1_knn_psns[id] = graph\n",
    "\n",
    "noise_knn_psns = {}\n",
    "for id, g in noise_74.items():\n",
    "    graph = knn(g, best_knn[\"k\"], best_knn[\"metric\"])\n",
    "    noise_knn_psns[id] = graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given Level 1 pseudobulks, generate noise and non-noise kernel-based PSNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "level1_kernel_psns = {}\n",
    "graphs = aff(list(level1_fm.values()), metric=best_kernel[\"metric\"], k=best_kernel[\"k\"], mu=best_kernel[\"mu\"])\n",
    "for i, id in enumerate(level1_fm.keys()):\n",
    "    level1_kernel_psns[id] = graphs[i]\n",
    "\n",
    "noise_kernel_psns = {}\n",
    "graphs_noise = [g.X.copy() for g in noise_74.values()]\n",
    "graphs_noise = compute.make_affinity(graphs_noise, metric=best_kernel[\"metric\"], K=best_kernel[\"k\"], mu=best_kernel[\"mu\"], normalize = False)\n",
    "for i, id in enumerate(noise_74.keys()):\n",
    "    noise_kernel_psns[id] = graphs_noise[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "level1_correlations = {}\n",
    "for id, ad_obj in level1_graphs.items():\n",
    "    level1_correlations[id] = ad_obj.to_df().T.corr().to_numpy()\n",
    "\n",
    "noise_correlations = {}\n",
    "for id, ad_obj in noise_74.items():\n",
    "    noise_correlations[id] = ad_obj.to_df().T.corr().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in noise_kernel_psns.keys():\n",
    "    np.random.shuffle(noise_kernel_psns[id])\n",
    "    np.random.shuffle(noise_knn_psns[id])\n",
    "    np.random.shuffle(noise_correlations[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(level1, 'correlations.pickle'), 'wb') as f:\n",
    "    pickle.dump(level1_correlations, f)\n",
    "with open(os.path.join(level1, 'knn_PSNs.pickle'), 'wb') as f:\n",
    "    pickle.dump(level1_knn_psns, f)\n",
    "with open(os.path.join(level1, 'kernel_PSNs.pickle'), 'wb') as f:\n",
    "    pickle.dump(level1_kernel_psns, f)\n",
    "\n",
    "\n",
    "with open(os.path.join(level1, 'noise_correlations.pickle'), 'wb') as f:\n",
    "    pickle.dump(noise_correlations, f)\n",
    "with open(os.path.join(level1, 'noise_knn_PSNs.pickle'), 'wb') as f:\n",
    "    pickle.dump(noise_knn_psns, f)\n",
    "with open(os.path.join(level1, 'noise_kernel_PSNs.pickle'), 'wb') as f:\n",
    "    pickle.dump(noise_kernel_psns, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kernel_psns = list(noise_kernel_psns.values()) + list(level1_kernel_psns.values())\n",
    "all_knn_psns = list(noise_knn_psns.values()) + list(level1_knn_psns.values()) \n",
    "all_feature_matrices = [noise.X for noise in noise_74.values()] + list(level1_fm.values()) \n",
    "all_correlations = list(noise_correlations.values()) + list(level1_correlations.values())\n",
    "all_names = list(noise_kernel_psns.keys()) + list(level1_kernel_psns.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {\"knn_psns\": all_knn_psns, \"kernel_psns\": all_kernel_psns, \"feature_matrices\": all_feature_matrices,  \"correlations\": all_correlations, \"names\":all_names}\n",
    "\n",
    "with open(os.path.join(level1, 'noise_and_none_noise_combined.pickle'), 'wb') as f:\n",
    "    pickle.dump(all_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "level2_kernel_psns = {}\n",
    "graphs = aff(list(level2_fm.values()), metric=best_kernel[\"metric\"], k=best_kernel[\"k\"], mu=best_kernel[\"mu\"])\n",
    "\n",
    "for i, id in enumerate(level2_fm.keys()):\n",
    "    level2_kernel_psns[id] = graphs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shakiba/Desktop/thesis.tmp/code/aggregated_patient_info/aggregated_patient_info/lib/python3.11/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "level2_knn_psns = {}\n",
    "        \n",
    "for id, g in level2_graphs.items():\n",
    "    graph = knn(g, best_knn[\"k\"], best_knn[\"metric\"])\n",
    "    level2_knn_psns[id] = graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "level2_correlations = {}\n",
    "for id, ad_obj in level2_graphs.items():\n",
    "    level2_correlations[id] = ad_obj.to_df().T.corr().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(level2_correlations) == len(level2_knn_psns) == len(level2_kernel_psns) == 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(level2, 'correlations.pickle'), 'wb') as f:\n",
    "    pickle.dump(level2_correlations, f)\n",
    "with open(os.path.join(level2, 'knn_PSNs.pickle'), 'wb') as f:\n",
    "    pickle.dump(level2_knn_psns, f)\n",
    "with open(os.path.join(level2, 'kernel_PSNs.pickle'), 'wb') as f:\n",
    "    pickle.dump(level2_kernel_psns, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "level3_kernel_psns = {}\n",
    "graphs = aff(list(level3_fm.values()), metric=best_kernel[\"metric\"], k=best_kernel[\"k\"], mu=best_kernel[\"mu\"])\n",
    "for i, id in enumerate(level3_graphs.keys()):\n",
    "    level3_kernel_psns[id] = graphs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shakiba/Desktop/thesis.tmp/code/aggregated_patient_info/aggregated_patient_info/lib/python3.11/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "level3_knn_psns = {}       \n",
    "for id, g in level3_graphs.items():\n",
    "    graph = knn(g, best_knn[\"k\"], best_knn[\"metric\"])\n",
    "    level3_knn_psns[id] = graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "level3_correlations = {}\n",
    "for id, ad_obj in level3_graphs.items():\n",
    "    level3_correlations[id] = ad_obj.to_df().T.corr().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(level3_correlations) == len(level3_knn_psns) == len(level3_kernel_psns) == len(level3_fm) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(level3, 'correlations.pickle'), 'wb') as f:\n",
    "    pickle.dump(level3_correlations, f)\n",
    "with open(os.path.join(level3, 'knn_PSNs.pickle'), 'wb') as f:\n",
    "    pickle.dump(level3_knn_psns, f)\n",
    "with open(os.path.join(level3, 'kernel_PSNs.pickle'), 'wb') as f:\n",
    "    pickle.dump(level3_kernel_psns, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Weights for the Weighted Average fusion\n",
    "\n",
    "uncomment the code snippets below to find and sace optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg_optimize(num_iter, psns):\n",
    "    adjusted_weights = {}\n",
    "\n",
    "    score_avg = {}\n",
    "    fused_graphs_avg = {}\n",
    "\n",
    "    for iteration in range(num_iter):\n",
    "        weights = {}\n",
    "        random_weights = np.random.uniform(low=0, high=10, size=len(psns))\n",
    "        random_weights = [w/ sum(random_weights) for w in random_weights]\n",
    "        \n",
    "        fused_network = 0\n",
    "        for i, id in enumerate(psns.keys()):\n",
    "            weights[id] = random_weights[i]\n",
    "            fused_network = fused_network + weights[id] * psns[id]\n",
    "          \n",
    "\n",
    "        transition_matrix = make_transition_matrix(fused_network)\n",
    "        d_s, dg_s, s = random_walker_objective(transition_matrix)\n",
    "    \n",
    "        fused_graphs_avg[iteration] = fused_network\n",
    "        score_avg[iteration] = s\n",
    "        adjusted_weights[iteration]  = weights\n",
    "    \n",
    "\n",
    "    score_avg = {k: v for k, v in sorted(score_avg.items(), key=lambda item: item[1])} # sort according to scores\n",
    "    best_iteration_avg = list(score_avg.keys())[-1]\n",
    "    best_weights_avg = adjusted_weights[best_iteration_avg]\n",
    "    best_fused_graph_avg = fused_graphs_avg[best_iteration_avg]\n",
    "    best_score_avg = score_avg[best_iteration_avg]\n",
    "\n",
    "    return best_score_avg, best_weights_avg, best_fused_graph_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_pns_score, best_psn_weights, best_wa_psn_graph = weighted_avg_optimize(100000, level1_knn_psns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_noise_score, best_noise_weights, best_wa_noise_graph = weighted_avg_optimize(100000, noise_knn_psns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_score_all, best_weights_all, best_wa_graph_all = weighted_avg_optimize(150000, {all_names[i]: all_knn_psns[i] for i in range(len(all_names))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_pns_score2, best_psn_weights2, best_wa_psn_graph2 = weighted_avg_optimize(100000, level2_knn_psns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_pns_score3, best_psn_weights3, best_wa_psn_graph3 = weighted_avg_optimize(100000, level3_knn_psns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modality_frequency_weights2 = {\"citeRNA\": 1/11, \"bulkRNA\": 1, \"adt\": 1/11, \"facs\": 1, \"luminex\": 1, \"cytof\": 1/12 }\\nmodality_frequency_weights2  = {id: modality_frequency_weights2[g.uns[\"modality\"]] * 1/6 for id, g in level2_graphs.items()}\\nweighted_avg_weights2 = {id: 1/len(level2_graphs)  for id, g in level2_graphs.items()}'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"modality_frequency_weights2 = {\"citeRNA\": 1/11, \"bulkRNA\": 1, \"adt\": 1/11, \"facs\": 1, \"luminex\": 1, \"cytof\": 1/12 }\n",
    "modality_frequency_weights2  = {id: modality_frequency_weights2[g.uns[\"modality\"]] * 1/6 for id, g in level2_graphs.items()}\n",
    "weighted_avg_weights2 = {id: 1/len(level2_graphs)  for id, g in level2_graphs.items()}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modality_frequency_weights3 = {id: 1/len(level3_graphs)  for id, g in level3_graphs.items()}\\nweighted_avg_weights3 = {id: 1/len(level2_graphs)  for id, g in level3_graphs.items()}'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"modality_frequency_weights3 = {id: 1/len(level3_graphs)  for id, g in level3_graphs.items()}\n",
    "weighted_avg_weights3 = {id: 1/len(level2_graphs)  for id, g in level3_graphs.items()}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with open(os.path.join(level1, 'weights_NOISE.pickle'), 'wb') as f:\\n    pickle.dump(best_noise_weights, f)\\nwith open(os.path.join(level1, 'weights_ALL.pickle'), 'wb') as f:\\n    pickle.dump(best_weights_all, f)\\n\\nwith open(os.path.join(level1, 'weights_modality_frequency.pickle'), 'wb') as f:\\n    pickle.dump(modality_frequency_weights, f)\\nwith open(os.path.join(level1, 'weights_uniform.pickle'), 'wb') as f:\\n    pickle.dump(weighted_avg_weights, f)\\nwith open(os.path.join(level1, 'weights_optimized.pickle'), 'wb') as f:\\n    pickle.dump(best_psn_weights, f)\\n\\nwith open(os.path.join(level2, 'weights_optimized.pickle'), 'wb') as f:\\n    pickle.dump(best_psn_weights2, f)\\nwith open(os.path.join(level2, 'weights_modality_frequency.pickle'), 'wb') as f:\\n    pickle.dump(modality_frequency_weights2, f)\\nwith open(os.path.join(level2, 'weights_uniform.pickle'), 'wb') as f:\\n    pickle.dump(weighted_avg_weights2, f)\\n\\n\\nwith open(os.path.join(level3, 'weights_optimized.pickle'), 'wb') as f:\\n    pickle.dump(best_psn_weights3, f)\\nwith open(os.path.join(level3, 'weights_modality_frequency.pickle'), 'wb') as f:\\n    pickle.dump(modality_frequency_weights3, f)\\nwith open(os.path.join(level3, 'weights_uniform.pickle'), 'wb') as f:\\n    pickle.dump(weighted_avg_weights3, f)\""
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"with open(os.path.join(level1, 'weights_NOISE.pickle'), 'wb') as f:\n",
    "    pickle.dump(best_noise_weights, f)\n",
    "with open(os.path.join(level1, 'weights_ALL.pickle'), 'wb') as f:\n",
    "    pickle.dump(best_weights_all, f)\n",
    "\n",
    "with open(os.path.join(level1, 'weights_modality_frequency.pickle'), 'wb') as f:\n",
    "    pickle.dump(modality_frequency_weights, f)\n",
    "with open(os.path.join(level1, 'weights_uniform.pickle'), 'wb') as f:\n",
    "    pickle.dump(weighted_avg_weights, f)\n",
    "with open(os.path.join(level1, 'weights_optimized.pickle'), 'wb') as f:\n",
    "    pickle.dump(best_psn_weights, f)\n",
    "\n",
    "with open(os.path.join(level2, 'weights_optimized.pickle'), 'wb') as f:\n",
    "    pickle.dump(best_psn_weights2, f)\n",
    "with open(os.path.join(level2, 'weights_modality_frequency.pickle'), 'wb') as f:\n",
    "    pickle.dump(modality_frequency_weights2, f)\n",
    "with open(os.path.join(level2, 'weights_uniform.pickle'), 'wb') as f:\n",
    "    pickle.dump(weighted_avg_weights2, f)\n",
    "\n",
    "\n",
    "with open(os.path.join(level3, 'weights_optimized.pickle'), 'wb') as f:\n",
    "    pickle.dump(best_psn_weights3, f)\n",
    "with open(os.path.join(level3, 'weights_modality_frequency.pickle'), 'wb') as f:\n",
    "    pickle.dump(modality_frequency_weights3, f)\n",
    "with open(os.path.join(level3, 'weights_uniform.pickle'), 'wb') as f:\n",
    "    pickle.dump(weighted_avg_weights3, f)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
